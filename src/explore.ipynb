{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models, Input\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.applications import EfficientNetB0\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dir = 'train_2'\n",
                "test_dir = 'test_2'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_size = 224\n",
                "batch_size = 64\n",
                "lr = 1e-4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalization(img):\n",
                "    img = img / 255.0  # Convierte a [0,1]\n",
                "    img = (img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]  # NormalizaciÃ³n ImageNet\n",
                "    return img\n",
                "\n",
                "train_datagen = ImageDataGenerator(\n",
                "    preprocessing_function=normalization,\n",
                "    validation_split=0.2,\n",
                "    rotation_range=15,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1,\n",
                "    horizontal_flip=True\n",
                ")\n",
                "\n",
                "test_datagen = ImageDataGenerator(\n",
                "    preprocessing_function=normalization\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'train_2'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_generator = \u001b[43mtrain_datagen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbinary\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m val_generator = train_datagen.flow_from_directory(\n\u001b[32m     11\u001b[39m     train_dir,\n\u001b[32m     12\u001b[39m     target_size=(img_size, img_size),\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m test_generator = test_datagen.flow_from_directory(\n\u001b[32m     20\u001b[39m     directory=os.path.dirname(test_dir),\n\u001b[32m     21\u001b[39m     classes=[os.path.basename(test_dir)],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     26\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[39m, in \u001b[36mImageDataGenerator.flow_from_directory\u001b[39m\u001b[34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflow_from_directory\u001b[39m(\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1122\u001b[39m     directory,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1136\u001b[39m     keep_aspect_ratio=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1137\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[39m, in \u001b[36mDirectoryIterator.__init__\u001b[39m\u001b[34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[32m    452\u001b[39m     classes = []\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(os.path.join(directory, subdir)):\n\u001b[32m    455\u001b[39m             classes.append(subdir)\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train_2'"
                    ]
                }
            ],
            "source": [
                "train_generator = train_datagen.flow_from_directory(\n",
                "    train_dir,\n",
                "    target_size=(img_size, img_size),\n",
                "    batch_size=batch_size,\n",
                "    class_mode='binary',\n",
                "    subset='training',\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "val_generator = train_datagen.flow_from_directory(\n",
                "    train_dir,\n",
                "    target_size=(img_size, img_size),\n",
                "    batch_size=batch_size,\n",
                "    class_mode='binary',\n",
                "    subset='validation',\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\n",
                "    directory=os.path.dirname(test_dir),\n",
                "    classes=[os.path.basename(test_dir)],\n",
                "    target_size=(img_size, img_size),\n",
                "    batch_size=1,\n",
                "    class_mode=None,\n",
                "    shuffle=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def inverse_normalize(img):\n",
                "    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
                "    img = np.clip(img, 0, 1)\n",
                "    return img\n",
                "\n",
                "# Ejemplo de visualizaciÃ³n:\n",
                "x, y = next(train_generator)\n",
                "plt.imshow(inverse_normalize(x[0]))\n",
                "plt.title(f\"Clase: {'Perro' if y[0] else 'Gato'}\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_model = EfficientNetB0(\n",
                "    weights='imagenet',\n",
                "    include_top=False,\n",
                "    input_shape=(img_size, img_size, 3)\n",
                ")\n",
                "base_model.trainable = False\n",
                "\n",
                "x = base_model.output\n",
                "x = GlobalAveragePooling2D()(x)\n",
                "x = Dropout(0.2)(x)\n",
                "x = Dense(128, activation='relu')(x)\n",
                "predictions = Dense(1, activation='sigmoid')(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Model(inputs=base_model.input, outputs=predictions)\n",
                "\n",
                "# CompilaciÃ³n con optimizador mejorado\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=lr),\n",
                "    loss='binary_crossentropy',\n",
                "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
                ")\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "early_stop = EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=5,\n",
                "    restore_best_weights=True\n",
                ")\n",
                "\n",
                "checkpoint = ModelCheckpoint(\n",
                "    'best_model.h5',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True\n",
                ")\n",
                "\n",
                "reduce_lr = ReduceLROnPlateau(\n",
                "    monitor='val_loss',\n",
                "    factor=0.2,\n",
                "    patience=2,\n",
                "    min_lr=1e-7\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fase 1: Entrenar solo las nuevas capas\n",
                "print(\"\\n--- FASE 1: Entrenamiento de capas nuevas ---\")\n",
                "history = model.fit(\n",
                "    train_generator,\n",
                "    epochs=15,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
